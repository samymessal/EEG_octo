{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783e24cf",
   "metadata": {},
   "source": [
    "\n",
    "# Sleep Spindle Study\n",
    "\n",
    "## Building Model\n",
    "\n",
    "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44cdde",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n",
    "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dceae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 19:38:10.113809: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 19:38:15.343750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-25 19:38:15.343913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-25 19:38:15.683605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-25 19:38:17.213635: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 19:38:17.254766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-25 19:38:47.566043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "import utils\n",
    "import feature_extraction\n",
    "import data_preparation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642274",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bae3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
      "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
      "Ready.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Annotations' has no attribute 'concatenate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata_preparation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset/train_S002_night1_hackathon_raw.mat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset/train_S003_night5_hackathon_raw.mat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset/train_S002_labeled.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset/train_S003_labeled.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSS0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSS1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minclude_entire_recording\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/files/data_preparation.py:117\u001b[0m, in \u001b[0;36mprocessed_data\u001b[0;34m(raw_filepaths, label_filepaths, labels, fmin, fmax, include_entire_recording)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Loop through each file path, prepare data, and collect epochs and labels\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m raw_filepath, label_filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(raw_filepaths, label_filepaths):\n\u001b[0;32m--> 117\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_entire_recording\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     all_epochs\u001b[38;5;241m.\u001b[39mappend(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    119\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mappend(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/app/files/data_preparation.py:98\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(filepath_raw, filepath_labels, labels, include_entire_recording)\u001b[0m\n\u001b[1;32m     96\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_entire_recording:\n\u001b[0;32m---> 98\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmk_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     labels_events, events \u001b[38;5;241m=\u001b[39m prepare_labels_events(labels_df, labels)\n",
      "File \u001b[0;32m/app/files/data_preparation.py:72\u001b[0m, in \u001b[0;36mmk_epochs\u001b[0;34m(raw, df, tmin, epoch_duration)\u001b[0m\n\u001b[1;32m     67\u001b[0m     annotations\u001b[38;5;241m.\u001b[39mappend(mne\u001b[38;5;241m.\u001b[39mAnnotations(onset\u001b[38;5;241m=\u001b[39m[onset],\n\u001b[1;32m     68\u001b[0m                                        duration\u001b[38;5;241m=\u001b[39m[epoch_duration],\n\u001b[1;32m     69\u001b[0m                                        description\u001b[38;5;241m=\u001b[39m[description]))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Combine all annotations\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m all_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnnotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m(annotations)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Attach Annotations to Raw Object\u001b[39;00m\n\u001b[1;32m     75\u001b[0m raw\u001b[38;5;241m.\u001b[39mset_annotations(all_annotations)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Annotations' has no attribute 'concatenate'"
     ]
    }
   ],
   "source": [
    "X, labels = data_preparation.processed_data([\"../dataset/train_S002_night1_hackathon_raw.mat\",\n",
    "                                            \"../dataset/train_S003_night5_hackathon_raw.mat\"\n",
    "                                            ],\n",
    "                                            [\"../dataset/train_S002_labeled.csv\",\n",
    "                                            \"../dataset/train_S003_labeled.csv\"\n",
    "                                            ],\n",
    "                                            labels=[\"SS0\", \"SS1\", \"K0\", \"K1\"],\n",
    "                                            fmin=11,\n",
    "                                            fmax=15,\n",
    "                                            include_entire_recording=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa572fe2",
   "metadata": {},
   "source": [
    "\n",
    "#### Model\n",
    "\n",
    "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Custom weighted binary cross-entropy loss function.\n",
    "    \"\"\"\n",
    "    # Convert weights to a TensorFlow tensor and ensure float32 data type\n",
    "    weights = tf.cast(weights, dtype=tf.float32)\n",
    "\n",
    "    # Ensure y_true and y_pred are of float32 type\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Clip predictions to prevent log(0) error\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    # Calculate Binary Cross Entropy\n",
    "    bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "    # Print the values of BCE\n",
    "    tf.print(\"BCE: \", bce, summarize=-1)  # summarize=-1 prints all values\n",
    "\n",
    "    # Apply weights\n",
    "    def apply_weights(args):\n",
    "        y_true_slice, bce_slice = args[0], args[1]\n",
    "        return bce_slice * tf.gather(weights, tf.cast(y_true_slice, tf.int32), axis=0)\n",
    "\n",
    "    # Apply weights using tf.map_fn\n",
    "    weighted_bce = tf.map_fn(apply_weights, (y_true, bce), dtype=tf.float32)\n",
    "\n",
    "    # Return mean loss\n",
    "    return K.mean(weighted_bce, axis=-1)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "for fold_no, (train, test) in enumerate(kfold.split(X)):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    weights = preprocess.compute_multi_label_loss_weights(labels[train])\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return weighted_binary_crossentropy(y_true, y_pred, weights)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=custom_loss, metrics=['accuracy'])\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = model.fit(X[train], labels[train], epochs=30)\n",
    "\n",
    "    perf_metrics = utils.evaluate_model(model, X[test], labels[test])\n",
    "    utils.save_model(model, history, perf_metrics, fold_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1999e83",
   "metadata": {},
   "source": [
    "### Visualize plots and metrics\n",
    "\n",
    "Determining the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94145278",
   "metadata": {},
   "source": [
    "Plot accuracies and loss for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c651cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"SS_bp4_35Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_0Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_detrend_Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_bp11_15Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "import os\n",
    "print(os.listdir(\"./ressources/models/metrics\"))\n",
    "filenames = [\n",
    "    \"SS_bp4_35Pre_0Features_LSTM_\",\n",
    "    \"SS_0Pre_0Features_LSTM_\",\n",
    "    \"SS_detrend_Pre_0Features_LSTM_\",\n",
    "    \"SS_bp11_15Pre_0Features_LSTM_\",\n",
    "    \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "]\n",
    "for filename in filenames:\n",
    "    utils.plot_fold_history(filename, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df47ce",
   "metadata": {},
   "source": [
    "Performance of each fold will be printed along with the average performance of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = utils.print_performances(\"SS_bp4_35Pre_0Features_LSTM_\", 1)\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
