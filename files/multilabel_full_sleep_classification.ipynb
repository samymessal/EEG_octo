{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samymessal/EEG_octo/blob/full_sleep_multi_label_classification/files/multilabel_full_sleep_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e24cf",
      "metadata": {
        "id": "783e24cf"
      },
      "source": [
        "\n",
        "# Sleep Spindle Study\n",
        "\n",
        "## Building Model\n",
        "\n",
        "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Ij62zkKwkMJ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij62zkKwkMJ6",
        "outputId": "da9fe57f-de1f-42e3-db61-0280acf150e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from mne) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from mne) (0.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.11.17)\n",
            "Requirement already satisfied: vmdpy in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vmdpy) (1.23.5)\n",
            "Collecting yasa\n",
            "  Downloading yasa-0.6.3.tar.gz (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yasa) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from yasa) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from yasa) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from yasa) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from yasa) (0.12.2)\n",
            "Requirement already satisfied: mne>=0.23 in /usr/local/lib/python3.10/dist-packages (from yasa) (1.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from yasa) (0.58.1)\n",
            "Collecting outdated (from yasa)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting antropy (from yasa)\n",
            "  Downloading antropy-0.1.6.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from yasa) (1.2.2)\n",
            "Collecting tensorpac>=0.6.5 (from yasa)\n",
            "  Downloading tensorpac-0.6.5-py3-none-any.whl (423 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.6/423.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyriemann>=0.2.7 (from yasa)\n",
            "  Downloading pyriemann-0.5.tar.gz (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sleepecg>=0.5.0 (from yasa)\n",
            "  Downloading sleepecg-0.5.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lspopt (from yasa)\n",
            "  Downloading lspopt-1.3.0-py2.py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from yasa) (7.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from yasa) (1.3.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from yasa) (4.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (3.1.2)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from mne>=0.23->yasa) (0.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->yasa) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from sleepecg>=0.5.0->yasa) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from sleepecg>=0.5.0->yasa) (2.31.0)\n",
            "Collecting stochastic (from antropy->yasa)\n",
            "  Downloading stochastic-0.7.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->yasa) (0.41.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->yasa) (3.0.9)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated->yasa) (67.7.2)\n",
            "Collecting littleutils (from outdated->yasa)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->yasa) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->yasa) (3.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->yasa) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->yasa) (6.3.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->yasa)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->yasa) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne>=0.23->yasa) (4.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->yasa) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->sleepecg>=0.5.0->yasa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->sleepecg>=0.5.0->yasa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->sleepecg>=0.5.0->yasa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->sleepecg>=0.5.0->yasa) (2023.11.17)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->yasa) (6.5.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne>=0.23->yasa) (2.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->yasa) (0.8.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (5.5.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->yasa) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->yasa) (0.2.12)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (6.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.15.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->yasa) (2.21)\n",
            "Building wheels for collected packages: yasa, pyriemann, antropy, littleutils\n",
            "  Building wheel for yasa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yasa: filename=yasa-0.6.3-py3-none-any.whl size=33800414 sha256=90a46b7333dd02125f19a8df80d2432767eb93494c0c4fa511f2b399bf0d326d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/d2/c1/9c3e43c2514650c9fe3805d37ceba9ff5e2c54484c489b3ba7\n",
            "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyriemann: filename=pyriemann-0.5-py2.py3-none-any.whl size=107752 sha256=6d655497d4e6286d14f1b32309c01d068a18fba3b4f1978dfabf85eed5590c2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/86/79/622e9c1dc933dc088e287ebfaac5aa9bdc6a38a9db193ce1f1\n",
            "  Building wheel for antropy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antropy: filename=antropy-0.1.6-py3-none-any.whl size=16880 sha256=4e6945658b51b20cacb255a584b7e1b73456b1952c3417a387ea9c6d447eaca3\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/22/06/e91d7bb213c7133d5e2eb34258623e1e19928d5f05e1ee6812\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=32c666fc604e4aad9b3733f4938f0ad733a343c8df6315ee118fc4a3aaff7e9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built yasa pyriemann antropy littleutils\n",
            "Installing collected packages: littleutils, jedi, tensorpac, stochastic, sleepecg, outdated, lspopt, pyriemann, antropy, yasa\n",
            "Successfully installed antropy-0.1.6 jedi-0.19.1 littleutils-0.2.2 lspopt-1.3.0 outdated-0.2.2 pyriemann-0.5 sleepecg-0.5.6 stochastic-0.7.0 tensorpac-0.6.5 yasa-0.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install mne\n",
        "!pip install vmdpy\n",
        "!pip install yasa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YlBkcCPRkoy9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlBkcCPRkoy9",
        "outputId": "fb3a26e7-8bed-4fa2-b808-d5a9698e9197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EEG_octo'...\n",
            "remote: Enumerating objects: 299, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 299 (delta 44), reused 68 (delta 20), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (299/299), 510.52 MiB | 17.44 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Updating files: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b full_sleep_multi_label_classification https://github.com/samymessal/EEG_octo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "-Q9vLtG3oXbh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q9vLtG3oXbh",
        "outputId": "39f9d464-d6a6-4386-93d0-b0367c2e1c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            ".ipynb_checkpoints\n",
            "utils.py\n",
            "Starting_kit_ntx_data_challenge_v0_1_Data_Exploration.ipynb\n",
            "preprocess.py\n",
            "feature_extraction.py\n",
            "data_loading.py\n",
            "multilabel_full_sleep_classification.ipynb\n",
            "data_preparation.py\n",
            "__pycache__\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/EEG_octo/files')\n",
        "\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "\n",
        "print(cwd)\n",
        "items = os.listdir('/content/EEG_octo/files')\n",
        "\n",
        "# Print the list of items\n",
        "for item in items:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c44cdde",
      "metadata": {
        "id": "2c44cdde"
      },
      "source": [
        "\n",
        "## Imports\n",
        "\n",
        "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "31dceae2",
      "metadata": {
        "id": "31dceae2"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "import data_preparation\n",
        "import preprocess\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import json\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, Dense, BatchNormalization, Flatten, LayerNormalization\n",
        "import tensorflow.keras.layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.utils import timeseries_dataset_from_array\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import detrend\n",
        "import yasa\n",
        "from scipy.signal import welch\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Flatten, Dense, Input, concatenate, Lambda\n",
        "from tensorflow.keras import Model, regularizers\n",
        "\n",
        "\n",
        "DEFAULT_DIVIDER = 10000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5642274",
      "metadata": {
        "id": "f5642274"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "18bae3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18bae3b7",
        "outputId": "32a17abe-51df-4289-cd31-9bb4fa335748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
            "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
            "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 1.60 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 1.60 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 1.60 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 1.60 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 1.60 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4965399)\n",
            "(4965399,)\n",
            "time_serie.shape: (4965399, 6)\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=5772730\n",
            "    Range : 0 ... 5772729 =      0.000 ... 23090.916 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=5772730\n",
            "    Range : 0 ... 5772729 =      0.000 ... 23090.916 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 20.92 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 20.92 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 20.92 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 20.92 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 20.92 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5772730)\n",
            "(5772730,)\n",
            "time_serie.shape: (5772730, 6)\n",
            "concat_time_serie.shape: (10738129, 6)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "def load_eeg_data(mat_file_path):\n",
        "    # Load the .mat file using scipy\n",
        "    mat = loadmat(mat_file_path)\n",
        "    # Extract EEG data\n",
        "    return mat['EEG'][0, 0]['data']\n",
        "\n",
        "def mk_raw_obj(eeg_data, sfreq=250):\n",
        "    info = mne.create_info(\n",
        "        ch_names=[f'EEG{i}' for i in range(len(eeg_data))],\n",
        "        sfreq=sfreq,\n",
        "        ch_types=['eeg' for _ in range(len(eeg_data))]\n",
        "    )\n",
        "\n",
        "    return mne.io.RawArray(eeg_data, info)\n",
        "\n",
        "def load_data(file_path, labels_path):\n",
        "    raw_mat = load_eeg_data(file_path)\n",
        "    raw = mk_raw_obj(raw_mat)\n",
        "    raw_data = raw.get_data()\n",
        "    labels = pd.read_csv(labels_path)\n",
        "    labels.sort_values(\"Timestamp\", inplace=True)\n",
        "    return raw, raw_data, labels\n",
        "\n",
        "def preprocess_recording_data(recording_data, frequency_band=None, sampling_freq=250):\n",
        "    # Detrending\n",
        "    recording_data = detrend(recording_data)\n",
        "    # band pass filtering\n",
        "    raw_obj = mk_raw_obj(recording_data, sfreq=sampling_freq)\n",
        "    bp_filter_raw_obj = raw_obj.filter(frequency_band[0], frequency_band[1], verbose=0)\n",
        "    recording_data = bp_filter_raw_obj.get_data()\n",
        "\n",
        "    return recording_data\n",
        "\n",
        "def hypnogram_propas(recording_data, sampling_freq=250):\n",
        "    \"\"\"\n",
        "    Computes the propabilites of the each sleep stages at each 30s epoch.\n",
        "    Then, upsamples the probabilites to match the shape of the recording.\n",
        "    ### Parameters:\n",
        "    recording_data: ndarray of the recording\n",
        "    ### Returns:\n",
        "    Tuple of shape four, each item is a 1D array of the probability of a sleep stage at a given timestamp.\n",
        "    Four for the four sleep stages: awake, REM, NREM1, NREM2, NREM3\n",
        "    \"\"\"\n",
        "    # For some reason, yasa doesn't work properly with the unscaled data.\n",
        "    scalled_raw_obj = mk_raw_obj(recording_data / DEFAULT_DIVIDER, sfreq=sampling_freq)\n",
        "    sls = yasa.SleepStaging(scalled_raw_obj, eeg_name=\"EEG0\")\n",
        "    hypno_proba = sls.predict_proba()\n",
        "    return [yasa.hypno_upsample_to_data(hypno_proba[column], 1/30, scalled_raw_obj, verbose=False) for column in hypno_proba.columns]\n",
        "\n",
        "def band_psd_ratio(recording_data, band1, band2, window_size, sfreq=250):\n",
        "    recording_data = recording_data.squeeze()\n",
        "    num_windows = len(recording_data) - window_size + 1\n",
        "    print(num_windows)\n",
        "    print(len(recording_data))\n",
        "    print(\"recording_data.shape:\", recording_data.shape)\n",
        "    print(\"recording_data.squeeze.shape:\", recording_data.squeeze().shape)\n",
        "    print(window_size)\n",
        "    power_ratios = np.empty((num_windows))\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        f, psd = welch(recording_data[i:i+window_size].squeeze(), sfreq, nperseg=int(sfreq * 2))\n",
        "        # Calculate power in the designated frequency bands\n",
        "        band1_power = psd[(f >= band1[0]) & (f <= band1[1])].mean()\n",
        "        band2_power = psd[(f >= band2[0]) & (f <= band2[1])].mean()\n",
        "        ratio = band1_power / band2_power\n",
        "        power_ratios[i] = ratio\n",
        "\n",
        "    power_ratios = np.pad(\n",
        "        power_ratios,\n",
        "        pad_width=(0, len(recording_data) - num_windows),\n",
        "        mode='constant',\n",
        "        constant_values=(power_ratios[0], power_ratios[-1]))\n",
        "    return power_ratios\n",
        "\n",
        "\n",
        "def dataset_from_files(\n",
        "        recording_files,\n",
        "        labels_files=None,\n",
        "        target_label=None,\n",
        "        sampling_freq=250,\n",
        "        frequency_band=None,\n",
        "        include_hypno_proba=True,\n",
        "        window_size_in_seconds=None,\n",
        "        band1=None,\n",
        "        band2=None,\n",
        "        shuffle=False\n",
        "        ):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the EEG recordings.\n",
        "    Returns a dataset keras obj.\n",
        "\n",
        "    ### Parameters:\n",
        "    recording_files: List of tuples of (.mat single channel eeg_recording\n",
        "    labels_files: .csv recording labels\n",
        "    sampling_freq: sampling frequency of the recording.\n",
        "    target_label: target label\n",
        "    frequency_band: tuple (min frequency, max frequency), if not None, used to band pass filter the recordings\n",
        "\n",
        "    ### Returns:\n",
        "    Timeseries dataset keras obj\n",
        "    \"\"\"\n",
        "    window_size = int(window_size_in_seconds * sampling_freq)\n",
        "    time_series = []\n",
        "    for recording_file in recording_files:\n",
        "        recording_data = load_eeg_data(recording_file)\n",
        "        preprocessed_recording_data = preprocess_recording_data(recording_data, frequency_band=frequency_band)\n",
        "        hypno_propas = hypnogram_propas(recording_data, sampling_freq=sampling_freq) if include_hypno_proba else ()\n",
        "        # psd_ratio = band_psd_ratio(recording_data, band1, band2, window_size) if window_size is not None else ()\n",
        "        print(preprocessed_recording_data.shape)\n",
        "        print(hypno_propas[0].shape)\n",
        "        time_serie = np.column_stack((\n",
        "            preprocessed_recording_data.squeeze(),\n",
        "            *hypno_propas,\n",
        "            # *psd_ratio\n",
        "            ))\n",
        "        print(\"time_serie.shape:\", time_serie.shape)\n",
        "        time_series.append(time_serie)\n",
        "    concat_time_serie = np.concatenate(time_series)\n",
        "    print(\"concat_time_serie.shape:\", concat_time_serie.shape)\n",
        "\n",
        "    if labels_files is not None:\n",
        "        assert target_label is not None, \"labels_files was set but not target_label.\"\n",
        "        target_arrays = []\n",
        "        for time_serie, labels_file in zip(time_series, labels_files):\n",
        "            labels_df = pd.read_csv(labels_file)\n",
        "            presence_incdices = labels_df[labels_df[target_label] == 1]['Timestamp']\n",
        "            target_array = np.zeros(time_serie.shape[0])\n",
        "            target_array[presence_incdices] = 1\n",
        "            target_arrays.append(target_array)\n",
        "\n",
        "        concat_target_array = np.concatenate(target_arrays)\n",
        "        print(concat_target_array[:10])\n",
        "    else:\n",
        "        concat_target_array = None\n",
        "\n",
        "\n",
        "\n",
        "    return timeseries_dataset_from_array(concat_time_serie, concat_target_array, window_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "\n",
        "dataset = dataset_from_files(\n",
        "    [\"/content/EEG_octo/dataset/train_S002_night1_hackathon_raw.mat\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_night5_hackathon_raw.mat\"\n",
        "    ],\n",
        "    [\"/content/EEG_octo/dataset/train_S002_labeled.csv\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_labeled.csv\"\n",
        "    ],\n",
        "    target_label=\"SS1\",\n",
        "    frequency_band=(8, 16),\n",
        "    window_size_in_seconds=2.5,\n",
        "    band1=(8, 13),\n",
        "    band2= (13, 16),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa572fe2",
      "metadata": {
        "id": "aa572fe2"
      },
      "source": [
        "\n",
        "#### Model\n",
        "\n",
        "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d58c9783",
      "metadata": {
        "id": "d58c9783"
      },
      "outputs": [],
      "source": [
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "        self.f1_score = self.add_weight(name='f1', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        self.f1_score.assign(2 * ((p * r) / (p + r + tf.keras.backend.epsilon())))\n",
        "\n",
        "    def result(self):\n",
        "        return self.f1_score\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "        self.f1_score.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb",
      "metadata": {
        "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb"
      },
      "outputs": [],
      "source": [
        "window_size = int(2.5 * 250)\n",
        "\n",
        "def create_model():\n",
        "    input_layer = keras.Input(shape=(window_size, 6))\n",
        "\n",
        "    # Input layer for the EEG time series\n",
        "    input_eeg = Lambda(lambda y: y[:, :, 0:2])(input_layer)\n",
        "    print(tf.shape(input_layer, out_type=None, name=None))\n",
        "    print(tf.shape(input_eeg, out_type=None, name=None))\n",
        "\n",
        "    # Layer normalization for EEG\n",
        "    norm_eeg = LayerNormalization()(input_eeg)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=32, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(norm_eeg)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=64, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=128, kernel_size=5, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Now you can flatten the output if you haven't applied global pooling before\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Input layer for the other features\n",
        "    first_elements = Lambda(lambda y: y[:, 0, 2:])(input_layer)\n",
        "    # Concatenate the CNN output and the first elements of the other features\n",
        "    concatenated = concatenate([x, first_elements])\n",
        "\n",
        "    x = Dense(\n",
        "        2048, activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(concatenated)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(\n",
        "        1024,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(\n",
        "        128,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
        "outputId": "5e80c664-d3c1-4238-9731-cef61145e53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: (128, 625, 6)\n",
            "Shape of y: (128,)\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 625, 6], name='tf.compat.v1.shape_6/Shape:0', description=\"created by layer 'tf.compat.v1.shape_6'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 625, 2], name='tf.compat.v1.shape_7/Shape:0', description=\"created by layer 'tf.compat.v1.shape_7'\")\n",
            "Epoch 1/30\n",
            " 4273/83887 [>.............................] - ETA: 3:04:02 - loss: 0.3672 - accuracy: 0.9998 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - f1_score: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "# kfold = KFold(n_splits=3, shuffle=True)\n",
        "# for fold_no, (train, test) in enumerate(kfold.split(X, labels)):\n",
        "#     print(\"train indices:\", train.shape)\n",
        "#     print(\"test indices:\", test.shape)\n",
        "#     # Define the model architecture\n",
        "#     model = create_model()\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(\n",
        "#         optimizer=keras.optimizers.Adam(),\n",
        "#         loss=\"binary_crossentropy\",\n",
        "#         metrics=[\n",
        "#             'accuracy',\n",
        "#             tf.keras.metrics.Precision(),\n",
        "#             tf.keras.metrics.Recall(),\n",
        "#             F1Score(),\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     # Train the model\n",
        "#     history = model.fit(\n",
        "#         X[train],\n",
        "#         labels[train],\n",
        "#         epochs=30,\n",
        "#         validation_data=(X[test], labels[test]),\n",
        "#     )\n",
        "\n",
        "\n",
        "#     training_f1_scores = history.history['f1_score']\n",
        "#     validation_f1_scores = history.history['val_f1_score']\n",
        "\n",
        "#     plt.plot(training_f1_scores, label='Training F1 Score')\n",
        "#     plt.plot(validation_f1_scores, label='Validation F1 Score')\n",
        "#     plt.xlabel('Epochs')\n",
        "#     plt.ylabel('F1 Score')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "for x, y in dataset.take(1):\n",
        "    print(\"Shape of x:\", x.shape)\n",
        "    print(\"Shape of y:\", y.shape)\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall(),\n",
        "        F1Score(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=30\n",
        ")\n",
        "\n",
        "\n",
        "training_f1_scores = history.history['f1_score']\n",
        "validation_f1_scores = history.history['val_f1_score']\n",
        "\n",
        "plt.plot(training_f1_scores, label='Training F1 Score')\n",
        "plt.plot(validation_f1_scores, label='Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}