{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783e24cf",
   "metadata": {},
   "source": [
    "\n",
    "# Sleep Spindle Study\n",
    "\n",
    "## Building Model\n",
    "\n",
    "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44cdde",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n",
    "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dceae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "import utils\n",
    "import feature_extraction\n",
    "import data_preparation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642274",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bae3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = data_preparation.processed_data([\"../dataset/train_S002_night1_hackathon_raw.mat\",\n",
    "                                            \"../dataset/train_S003_night5_hackathon_raw.mat\"\n",
    "                                            ],\n",
    "                                            [\"../dataset/train_S002_labeled.csv\",\n",
    "                                            \"../dataset/train_S003_labeled.csv\"\n",
    "                                            ],\n",
    "                                            labels=[\"SS0\", \"SS1\"],\n",
    "                                            fmin=11,\n",
    "                                            fmax=15,\n",
    "                                            include_entire_recording=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa572fe2",
   "metadata": {},
   "source": [
    "\n",
    "#### Model\n",
    "\n",
    "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Custom weighted binary cross-entropy loss function.\n",
    "    \"\"\"\n",
    "    # Convert weights to a TensorFlow tensor and ensure float32 data type\n",
    "    weights = tf.cast(weights, dtype=tf.float32)\n",
    "\n",
    "    # Ensure y_true and y_pred are of float32 type\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Clip predictions to prevent log(0) error\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    # Calculate Binary Cross Entropy\n",
    "    bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "    # Apply weights\n",
    "    def apply_weights(args):\n",
    "        y_true_slice, bce_slice = args[0], args[1]\n",
    "        return bce_slice * tf.gather(weights, tf.cast(y_true_slice, tf.int32), axis=0)\n",
    "\n",
    "    # Apply weights using tf.map_fn\n",
    "    weighted_bce = tf.map_fn(apply_weights, (y_true, bce), dtype=tf.float32)\n",
    "\n",
    "    # Return mean loss\n",
    "    return K.mean(weighted_bce, axis=-1)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "for fold_no, (train, test) in enumerate(kfold.split(X)):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    weights = preprocess.compute_multi_label_loss_weights(labels[train])\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return weighted_binary_crossentropy(y_true, y_pred, weights)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = model.fit(X[train], labels[train], epochs=30)\n",
    "\n",
    "    perf_metrics = utils.evaluate_model(model, X[test], labels[test])\n",
    "    utils.save_model(model, history, perf_metrics, fold_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1999e83",
   "metadata": {},
   "source": [
    "### Visualize plots and metrics\n",
    "\n",
    "Determining the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94145278",
   "metadata": {},
   "source": [
    "Plot accuracies and loss for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c651cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"SS_bp4_35Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_0Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_detrend_Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_bp11_15Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "import os\n",
    "print(os.listdir(\"./ressources/models/metrics\"))\n",
    "filenames = [\n",
    "    \"SS_bp4_35Pre_0Features_LSTM_\",\n",
    "    \"SS_0Pre_0Features_LSTM_\",\n",
    "    \"SS_detrend_Pre_0Features_LSTM_\",\n",
    "    \"SS_bp11_15Pre_0Features_LSTM_\",\n",
    "    \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "]\n",
    "for filename in filenames:\n",
    "    utils.plot_fold_history(filename, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df47ce",
   "metadata": {},
   "source": [
    "Performance of each fold will be printed along with the average performance of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = utils.print_performances(\"SS_bp4_35Pre_0Features_LSTM_\", 1)\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
