{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783e24cf",
   "metadata": {},
   "source": [
    "\n",
    "# Sleep Spindle Study\n",
    "\n",
    "## Building Model\n",
    "\n",
    "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44cdde",
   "metadata": {},
   "source": [
    "\n",
    "## Imports\n",
    "\n",
    "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dceae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 22:30:59.580979: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 22:31:00.092440: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-23 22:31:00.092825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-23 22:31:00.165640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-23 22:31:00.418520: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 22:31:00.422254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 22:31:02.965529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "import utils\n",
    "import feature_extraction\n",
    "import data_preparation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642274",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bae3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
      "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
      "Ready.\n",
      "event_labels_series.shape: (1191,)\n",
      "np.unique(event_labels_series, return_counts=True): (array([1]), array([1191]))\n",
      "Not setting metadata\n",
      "1191 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1191 events and 626 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/files/utils.py:43: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  data = epochs.get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1191 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/files/feature_extraction.py:13: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  for epoch in epochs.get_data():\n",
      "/app/files/feature_extraction.py:27: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  for epoch in epochs.get_data():\n",
      "/app/files/feature_extraction.py:45: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  eeg_data = epochs.get_data()\n",
      "/app/files/feature_extraction.py:58: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  raw_data = epochs.get_data()\n"
     ]
    }
   ],
   "source": [
    "X, labels = data_preparation.processed_data([\"../dataset/train_S002_night1_hackathon_raw.mat\",\n",
    "                                            #\"../dataset/train_S003_night5_hackathon_raw.mat\"\n",
    "                                            ],\n",
    "                                            [\"../dataset/train_S002_labeled.csv\",\n",
    "                                            #\"../dataset/train_S003_labeled.csv\"\n",
    "                                            ],\n",
    "                                            labels=[\"SS0\", \"SS1\"],\n",
    "                                            fmin=11,\n",
    "                                            fmax=15,\n",
    "                                            include_entire_recording=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa572fe2",
   "metadata": {},
   "source": [
    "\n",
    "#### Model\n",
    "\n",
    "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58c9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89811321 1.12796209]\n",
      " [1.12796209 0.89811321]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 22:37:35.882931: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 95352320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_1968/899044633.py\", line 55, in custom_loss  *\n        return weighted_binary_crossentropy(y_true, y_pred, weights)\n    File \"/tmp/ipykernel_1968/899044633.py\", line 34, in weighted_binary_crossentropy  *\n        weighted_bce = tf.map_fn(apply_weights, (y_true, bce), dtype=tf.float32)\n\n    TypeError: Could not build a TypeSpec for name: \"custom_loss/PrintV2\"\n    op: \"PrintV2\"\n    input: \"custom_loss/StringFormat\"\n    attr {\n      key: \"output_stream\"\n      value {\n        s: \"stderr\"\n      }\n    }\n    attr {\n      key: \"end\"\n      value {\n        s: \"\\n\"\n      }\n    }\n     of unsupported type <class 'tensorflow.python.framework.ops.Operation'>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39mcustom_loss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Fit data to model\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m perf_metrics \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mevaluate_model(model, X[test], labels[test])\n\u001b[1;32m     64\u001b[0m utils\u001b[38;5;241m.\u001b[39msave_model(model, history, perf_metrics, fold_no)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filen3nlgjyq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea9h8pzph.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__custom_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(weighted_binary_crossentropy), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(weights)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file1ctmdt10.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__weighted_binary_crossentropy\u001b[0;34m(y_true, y_pred, weights)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fscope_1\u001b[38;5;241m.\u001b[39mret(retval__1, do_return_1)\n\u001b[0;32m---> 33\u001b[0m weighted_bce \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_1968/899044633.py\", line 55, in custom_loss  *\n        return weighted_binary_crossentropy(y_true, y_pred, weights)\n    File \"/tmp/ipykernel_1968/899044633.py\", line 34, in weighted_binary_crossentropy  *\n        weighted_bce = tf.map_fn(apply_weights, (y_true, bce), dtype=tf.float32)\n\n    TypeError: Could not build a TypeSpec for name: \"custom_loss/PrintV2\"\n    op: \"PrintV2\"\n    input: \"custom_loss/StringFormat\"\n    attr {\n      key: \"output_stream\"\n      value {\n        s: \"stderr\"\n      }\n    }\n    attr {\n      key: \"end\"\n      value {\n        s: \"\\n\"\n      }\n    }\n     of unsupported type <class 'tensorflow.python.framework.ops.Operation'>.\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Custom weighted binary cross-entropy loss function.\n",
    "    \"\"\"\n",
    "    # Convert weights to a TensorFlow tensor and ensure float32 data type\n",
    "    weights = tf.cast(weights, dtype=tf.float32)\n",
    "\n",
    "    # Ensure y_true and y_pred are of float32 type\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Clip predictions to prevent log(0) error\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    # Calculate Binary Cross Entropy\n",
    "    bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "    # Print the values of BCE\n",
    "    tf.print(\"BCE: \", bce, summarize=-1)  # summarize=-1 prints all values\n",
    "\n",
    "    # Apply weights\n",
    "    def apply_weights(args):\n",
    "        y_true_slice, bce_slice = args[0], args[1]\n",
    "        return bce_slice * tf.gather(weights, tf.cast(y_true_slice, tf.int32), axis=0)\n",
    "\n",
    "    # Apply weights using tf.map_fn\n",
    "    weighted_bce = tf.map_fn(apply_weights, (y_true, bce), dtype=tf.float32)\n",
    "\n",
    "    # Return mean loss\n",
    "    return K.mean(weighted_bce, axis=-1)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "for fold_no, (train, test) in enumerate(kfold.split(X)):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    weights = preprocess.compute_multi_label_loss_weights(labels[train])\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        return weighted_binary_crossentropy(y_true, y_pred, weights)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=custom_loss, metrics=['accuracy'])\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = model.fit(X[train], labels[train], epochs=30)\n",
    "\n",
    "    perf_metrics = utils.evaluate_model(model, X[test], labels[test])\n",
    "    utils.save_model(model, history, perf_metrics, fold_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1999e83",
   "metadata": {},
   "source": [
    "### Visualize plots and metrics\n",
    "\n",
    "Determining the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94145278",
   "metadata": {},
   "source": [
    "Plot accuracies and loss for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c651cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"SS_bp4_35Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_0Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_detrend_Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_bp11_15Pre_0Features_LSTM_\"\n",
    "# filename = \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "import os\n",
    "print(os.listdir(\"./ressources/models/metrics\"))\n",
    "filenames = [\n",
    "    \"SS_bp4_35Pre_0Features_LSTM_\",\n",
    "    \"SS_0Pre_0Features_LSTM_\",\n",
    "    \"SS_detrend_Pre_0Features_LSTM_\",\n",
    "    \"SS_bp11_15Pre_0Features_LSTM_\",\n",
    "    \"SS_VDM1_3Pre_0Features_LSTM_\"\n",
    "]\n",
    "for filename in filenames:\n",
    "    utils.plot_fold_history(filename, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df47ce",
   "metadata": {},
   "source": [
    "Performance of each fold will be printed along with the average performance of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = utils.print_performances(\"SS_bp4_35Pre_0Features_LSTM_\", 1)\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
