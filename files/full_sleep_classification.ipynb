{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samymessal/EEG_octo/blob/full_sleep_multi_label_classification/files/full_sleep_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e24cf",
      "metadata": {
        "id": "783e24cf"
      },
      "source": [
        "\n",
        "# Sleep Spindle Study\n",
        "\n",
        "## Building Model\n",
        "\n",
        "In this notebook, we build a model to detect the presence of sleep spindles in the entire EEG recording.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ij62zkKwkMJ6",
      "metadata": {
        "id": "Ij62zkKwkMJ6"
      },
      "outputs": [],
      "source": [
        "!pip install mne -q\n",
        "!pip install vmdpy -q\n",
        "!pip install yasa -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YlBkcCPRkoy9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlBkcCPRkoy9",
        "outputId": "6f0f6405-6777-4dd9-9347-b0865626b92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EEG_octo' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b full_sleep_multi_label_classification https://github.com/samymessal/EEG_octo\n",
        "import sys\n",
        "sys.path.append('/content/EEG_octo/files')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c44cdde",
      "metadata": {
        "id": "2c44cdde"
      },
      "source": [
        "\n",
        "## Imports\n",
        "\n",
        "We will import the necessary libraries that are needed for processing the data, building the model, and evaluating its performance.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31dceae2",
      "metadata": {
        "id": "31dceae2"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, BatchNormalization, Flatten, LayerNormalization, Lambda, concatenate\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "import data_preparation\n",
        "import preprocess\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import json\n",
        "from tensorflow.keras.metrics import Metric\n",
        "import tensorflow.keras.layers\n",
        "from keras.utils import timeseries_dataset_from_array\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import detrend\n",
        "import yasa\n",
        "from scipy.signal import welch\n",
        "from tensorflow.keras import Model, regularizers\n",
        "\n",
        "\n",
        "DEFAULT_DIVIDER = 10000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5642274",
      "metadata": {
        "id": "f5642274"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Using the `processed_data` function from the previous step to download our concatenated raw with its correspondent preprocessing and features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18bae3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18bae3b7",
        "outputId": "9957fcee-3f48-45d4-ccd3-882872bf6617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RawArray with float64 data, n_channels=1, n_times=4965399\n",
            "    Range : 0 ... 4965398 =      0.000 ... 19861.592 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=397232\n",
            "    Range : 0 ... 397231 =      0.000 ...  1588.924 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 28.93 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_timeserie.shape before : (397232,)\n",
            "windowed_time_serie.shape: (397183, 50, 6)\n",
            "target_timeserie.shape: (397183,)\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=5772730\n",
            "    Range : 0 ... 5772729 =      0.000 ... 23090.916 secs\n",
            "Ready.\n",
            "Creating RawArray with float64 data, n_channels=1, n_times=461818\n",
            "    Range : 0 ... 461817 =      0.000 ...  1847.268 secs\n",
            "Ready.\n",
            "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n",
            "WARNING:yasa:Hypnogram is SHORTER than data by 17.27 seconds. Padding hypnogram with last value to match data.size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_timeserie.shape before : (461818,)\n",
            "windowed_time_serie.shape: (461769, 50, 6)\n",
            "target_timeserie.shape: (461769,)\n",
            "concat_timeseries.shape: (858952, 50, 6)\n",
            "concat_target_timeseries.shape: (858952,)\n"
          ]
        }
      ],
      "source": [
        "def load_eeg_data(mat_file_path):\n",
        "    # Load the .mat file using scipy\n",
        "    mat = loadmat(mat_file_path)\n",
        "    # Extract EEG data\n",
        "    return mat['EEG'][0, 0]['data']\n",
        "\n",
        "def mk_raw_obj(eeg_data, sfreq=250):\n",
        "    info = mne.create_info(\n",
        "        ch_names=[f'EEG{i}' for i in range(len(eeg_data))],\n",
        "        sfreq=sfreq,\n",
        "        ch_types=['eeg' for _ in range(len(eeg_data))]\n",
        "    )\n",
        "\n",
        "    return mne.io.RawArray(eeg_data, info)\n",
        "\n",
        "def load_data(file_path, labels_path):\n",
        "    raw_mat = load_eeg_data(file_path)\n",
        "    raw = mk_raw_obj(raw_mat)\n",
        "    raw_data = raw.get_data()\n",
        "    labels = pd.read_csv(labels_path)\n",
        "    labels.sort_values(\"Timestamp\", inplace=True)\n",
        "    return raw, raw_data, labels\n",
        "\n",
        "def mne_events_from_labels_df(recording_raw_obj: mne.io.Raw, labels_df: pd.DataFrame, target_label: str,):\n",
        "    presence_mask = labels_df[target_label] == 1\n",
        "    nb_events = len(labels_df[presence_mask])\n",
        "\n",
        "    return np.column_stack((labels_df['Timestamp'][presence_mask], np.ones(nb_events), np.ones(nb_events)))\n",
        "\n",
        "\n",
        "def preprocess_data(recording_data,\n",
        "                    frequency_band,\n",
        "                    sampling_freq,\n",
        "                    resampling_freq,\n",
        "                    labels_df,\n",
        "                    target_label,\n",
        "                    window_size_in_seconds):\n",
        "    # Detrending\n",
        "    recording_data = detrend(recording_data)\n",
        "\n",
        "    # Create mne raw obj\n",
        "    recording_raw_obj = mk_raw_obj(recording_data)\n",
        "    events = mne_events_from_labels_df(recording_raw_obj, labels_df, target_label)\n",
        "\n",
        "    # Resampling\n",
        "    recording_raw_obj, events = recording_raw_obj.resample(resampling_freq, events=events)\n",
        "    labels_df = pd.DataFrame(events, columns=[\"Timestamp\", \"ignore\", target_label])\n",
        "\n",
        "    # # Band pass filtering\n",
        "    # bp_filter_raw_obj = recording_raw_obj.filter(frequency_band[0], frequency_band[1], verbose=0)\n",
        "\n",
        "    return recording_raw_obj, labels_df\n",
        "\n",
        "def hypnogram_propas(recording_raw_obj, sampling_freq=250):\n",
        "    \"\"\"\n",
        "    Computes the propabilites of the each sleep stages at each 30s epoch.\n",
        "    Then, upsamples the probabilites to match the shape of the recording.\n",
        "    ### Parameters:\n",
        "    recording_data: ndarray of the recording\n",
        "    ### Returns:\n",
        "    Tuple of shape four, each item is a 1D array of the probability of a sleep stage at a given timestamp.\n",
        "    Four for the four sleep stages: awake, REM, NREM1, NREM2, NREM3\n",
        "    \"\"\"\n",
        "    # For some reason, yasa doesn't work properly with the unscaled data.\n",
        "    scalled_raw_obj = mk_raw_obj(recording_raw_obj.get_data() / DEFAULT_DIVIDER, sfreq=sampling_freq)\n",
        "    sls = yasa.SleepStaging(scalled_raw_obj, eeg_name=\"EEG0\")\n",
        "    hypno_proba = sls.predict_proba()\n",
        "    return [yasa.hypno_upsample_to_data(hypno_proba[column], 1/30, scalled_raw_obj, verbose=False) for column in hypno_proba.columns]\n",
        "\n",
        "def window_data(data, window_size):\n",
        "    num_windows = len(data) - window_size + 1\n",
        "    return np.array([data[i:i+window_size] for i in range(num_windows)]), num_windows\n",
        "\n",
        "def dataset_from_files(\n",
        "        recording_files,\n",
        "        labels_files,\n",
        "        target_label,\n",
        "        resampling_freq,\n",
        "        sampling_freq,\n",
        "        frequency_band,\n",
        "        window_size_in_seconds,\n",
        "        shuffle=False,\n",
        "        batch_size=128\n",
        "        ):\n",
        "    time_series = []\n",
        "    target_timeseries = []\n",
        "    window_size = int(window_size_in_seconds * resampling_freq)\n",
        "    for recording_file, labels_file in zip(recording_files, labels_files):\n",
        "        # Loading\n",
        "        labels_df = pd.read_csv(labels_file)\n",
        "        recording_data = load_eeg_data(recording_file)\n",
        "\n",
        "        # Preprocessing\n",
        "        preprocessed_recording_raw_obj, labels_df = preprocess_data(\n",
        "            recording_data,\n",
        "            frequency_band,\n",
        "            sampling_freq,\n",
        "            resampling_freq,\n",
        "            labels_df,\n",
        "            target_label,\n",
        "            window_size_in_seconds,\n",
        "            )\n",
        "\n",
        "        # Feature engineering\n",
        "        hypno_propas = hypnogram_propas(preprocessed_recording_raw_obj, sampling_freq=sampling_freq)\n",
        "\n",
        "        # Features formatting\n",
        "        time_serie = np.column_stack((preprocessed_recording_raw_obj.get_data().squeeze(), *hypno_propas))\n",
        "        target_timeserie = np.zeros(time_serie.shape[0])\n",
        "        for timestamp in labels_df[\"Timestamp\"]:\n",
        "          target_timeserie[int(timestamp)] = 1\n",
        "        windowed_time_serie, num_windows = window_data(time_serie, window_size)\n",
        "        print(\"target_timeserie.shape before :\", target_timeserie.shape)\n",
        "        target_timeserie = target_timeserie[:windowed_time_serie.shape[0]]\n",
        "        print(\"windowed_time_serie.shape:\", windowed_time_serie.shape)\n",
        "        print(\"target_timeserie.shape:\", target_timeserie.shape)\n",
        "\n",
        "        time_series.append(windowed_time_serie)\n",
        "        target_timeseries.append(target_timeserie)\n",
        "\n",
        "    concat_timeseries = np.concatenate(time_series, axis=0)\n",
        "    concat_target_timeseries = np.concatenate(target_timeseries, axis=0)\n",
        "\n",
        "    print(\"concat_timeseries.shape:\", concat_timeseries.shape)\n",
        "    print(\"concat_target_timeseries.shape:\", concat_target_timeseries.shape)\n",
        "\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(concat_target_timeseries), y=concat_target_timeseries.ravel())\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((concat_timeseries, concat_target_timeseries))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(concat_timeseries))\n",
        "\n",
        "    return dataset, window_size, class_weights\n",
        "\n",
        "\n",
        "dataset, window_size, class_weights = dataset_from_files(\n",
        "    [\"/content/EEG_octo/dataset/train_S002_night1_hackathon_raw.mat\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_night5_hackathon_raw.mat\"\n",
        "    ],\n",
        "    [\"/content/EEG_octo/dataset/train_S002_labeled.csv\",\n",
        "    \"/content/EEG_octo/dataset/train_S003_labeled.csv\"\n",
        "    ],\n",
        "    sampling_freq=250,\n",
        "    resampling_freq=20,\n",
        "    target_label=\"SS1\",\n",
        "    frequency_band=(8, 16),\n",
        "    window_size_in_seconds=2.5,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa572fe2",
      "metadata": {
        "id": "aa572fe2"
      },
      "source": [
        "\n",
        "#### Model\n",
        "\n",
        "The chosen model is an LSTM, since we are dealing with timeframes, LSTM are known to deal well with time depending samples. A k-cross validation is implemented, partitioning the data into 5 parts and alterning between the 4 parts for training and the 1 for testing.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d58c9783",
      "metadata": {
        "id": "d58c9783"
      },
      "outputs": [],
      "source": [
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "        self.f1_score = self.add_weight(name='f1', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        self.f1_score.assign(2 * ((p * r) / (p + r + tf.keras.backend.epsilon())))\n",
        "\n",
        "    def result(self):\n",
        "        return self.f1_score\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "        self.f1_score.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb",
      "metadata": {
        "id": "062e6a1a-72af-4fe7-a1cc-ae81ace437eb"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    input_layer = keras.Input(shape=(window_size, 6))\n",
        "\n",
        "    # Input layer for the EEG time series\n",
        "    input_eeg = Lambda(lambda y: y[:, :, 0:2])(input_layer)\n",
        "\n",
        "    # Layer normalization for EEG\n",
        "    norm_eeg = LayerNormalization()(input_eeg)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=32, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(norm_eeg)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=64, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(\n",
        "        filters=128, kernel_size=5, strides=1, activation=\"relu\", padding=\"same\"\n",
        "    )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Now you can flatten the output if you haven't applied global pooling before\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Input layer for the other features\n",
        "    first_elements = Lambda(lambda y: y[:, 0, 2:])(input_layer)\n",
        "    # Concatenate the CNN output and the first elements of the other features\n",
        "    concatenated = concatenate([x, first_elements])\n",
        "\n",
        "    x = Dense(\n",
        "        2048, activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(concatenated)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(\n",
        "        1024,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(\n",
        "        128,\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "        bias_regularizer=regularizers.L2(1e-4),\n",
        "    )(x)\n",
        "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7637f27c-9443-47ce-87c3-879d60a2caa6",
        "outputId": "afe4b3c6-46ea-46a8-a871-a18b45f04840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: (128, 50, 6)\n",
            "Shape of y: (128,)\n",
            "Shape of x: (128, 50, 6)\n",
            "Shape of y: (128,)\n",
            "Shape of x: (128, 50, 6)\n",
            "Shape of y: (128,)\n",
            "Epoch 1/30\n",
            "3739/6711 [===============>..............] - ETA: 52s - loss: 5.4906 - accuracy: 0.5403 - precision: 0.0025 - recall: 0.7378 - f1_score: 0.0049"
          ]
        }
      ],
      "source": [
        "# for idx, elem in enumerate(dataset):\n",
        "#   print(f'batch #{idx}: {elem}')\n",
        "\n",
        "for x, y in dataset.take(3):\n",
        "    print(\"Shape of x:\", x.shape)\n",
        "    print(\"Shape of y:\", y.shape)\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(),\n",
        "        tf.keras.metrics.Recall(),\n",
        "        F1Score(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=30,\n",
        "    class_weight={label: weight for label, weight in enumerate(class_weights)}\n",
        ")\n",
        "\n",
        "\n",
        "training_f1_scores = history.history['f1_score']\n",
        "validation_f1_scores = history.history['val_f1_score']\n",
        "\n",
        "plt.plot(training_f1_scores, label='Training F1 Score')\n",
        "plt.plot(validation_f1_scores, label='Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}